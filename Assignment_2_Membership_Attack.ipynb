{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 2 - Membership Attack.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUFmbdXnvqCe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# required imports\n",
        "import sys \n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline  \n",
        "\n",
        "\n",
        "import torch\n",
        "import torchvision \n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data.sampler import SubsetRandomSampler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NuEdW5dvvBi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "974f3c27-2efc-4acd-c803-ca4733430f69"
      },
      "source": [
        "# mount the google drive to download the datasets\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#project_path = 'include pathe to where your notebook is located on the drive'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5eks4_4vvDa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create transforms to load the images, nothing much is needed here. \n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "# Normalize the test set same as training set without augmentation\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vALYTeuLvvFo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8332da8a-8956-4741-a4e4-945f0b4d4fe3"
      },
      "source": [
        "# download CIFAR 10 training set\n",
        "trainset = torchvision.datasets.CIFAR10('../data', train=True,\n",
        "                                        download=True, transform=transform_train)\n",
        "\n",
        "# load the trainning set\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True)\n",
        "\n",
        "# download the test data\n",
        "testset = torchvision.datasets.CIFAR10('../data', train=True,\n",
        "                                        download=True, transform=transform_test)\n",
        "\n",
        "# load the test data\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=4, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "# check those manually on the dataset site: https://www.cs.toronto.edu/~kriz/cifar.html "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFzDOb7uvvID",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "779fd381-7268-4899-91ff-333a124977f5"
      },
      "source": [
        "# divide the training dataset into the required groups Make sure they are balanced\n",
        "# original trainset is made of 50k images\n",
        "\n",
        "total_size = len(trainset)\n",
        "test_size = len(testset)\n",
        "#loader_size = len(trainloader)\n",
        "split1 = total_size // 4\n",
        "split2 = split1 * 2\n",
        "split3 = split1 * 3\n",
        "\n",
        "#print(loader_size)\n",
        "\n",
        "print(total_size, test_size, split1, split2, split3)\n",
        "\n",
        "indices = list(range(total_size))\n",
        "\n",
        "# two groups to train the shadow (in and out)\n",
        "shadow_train_idx = indices[:split1]\n",
        "shadow_out_idx = indices[split1:split2]\n",
        "\n",
        "# two groups to train the Target (in and out)\n",
        "target_train_idx = indices[split2:split3]\n",
        "target_out_idx = indices[split3:total_size]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000 50000 12500 25000 37500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrLiLvh0vvKh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_batch_size = 128 # pick your own\n",
        "test_batch_size = 64\n",
        "\n",
        "#train_batch_size = 128 # maybe only need one\n",
        "#test_batch_size = 64\n",
        "\n",
        "# divide and load shadow train in and out\n",
        "shadow_train_sampler = SubsetRandomSampler(shadow_train_idx) # Pytorch function\n",
        "shadow_train_loader = torch.utils.data.DataLoader(trainset, batch_size=train_batch_size, sampler=shadow_train_sampler)\n",
        "\n",
        "shadow_out_sampler = SubsetRandomSampler(shadow_out_idx)\n",
        "shadow_out_loader = torch.utils.data.DataLoader(testset, batch_size=test_batch_size, sampler=shadow_out_sampler)\n",
        "\n",
        "# divide and load Target in and out\n",
        "target_train_sampler = SubsetRandomSampler(target_train_idx)\n",
        "target_out_sampler = SubsetRandomSampler(target_out_idx)\n",
        "\n",
        "target_train_loader = torch.utils.data.DataLoader(trainset, batch_size=train_batch_size, sampler=target_train_sampler)\n",
        "\n",
        "target_out_loader = torch.utils.data.DataLoader(testset, batch_size=test_batch_size, sampler=target_out_sampler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39TpDxe8vvM9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a CNN\n",
        "# Input shape (3, 32, 32) \n",
        "# architecture: simple. 2 conv and 2 Max pool, followed by 2 fc (120, 84) \n",
        "# output of fc is 10 because we have 10 classes!\n",
        "\n",
        "\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    \"\"\"CNN.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"CNN Builder.\"\"\"\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.conv_layer = nn.Sequential(\n",
        "\n",
        "            # Conv Layer block 1\n",
        "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Conv Layer block 2\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout2d(p=0.05),\n",
        "\n",
        "        )\n",
        "\n",
        "\n",
        "        self.fc_layer = nn.Sequential(\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.Linear(8192, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.1),\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Perform forward.\"\"\"\n",
        "        \n",
        "        # conv layers\n",
        "        x = self.conv_layer(x)\n",
        "        \n",
        "        # flatten\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        # fc layer\n",
        "        x = self.fc_layer(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YrgFcjOvvPm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "e593e2c1-0f03-4f29-f55b-de2faaba4624"
      },
      "source": [
        "# initalize a target model and train it\n",
        "epochs = 20\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "target_model = CNN()\n",
        "target_model.to(device)\n",
        "criterion = nn.CrossEntropyLoss() # CrossEntropyLoss\n",
        "optimizer = optim.Adam(target_model.parameters(), lr = .001)# try Adam VS SGD\n",
        "\n",
        "# let the magic begin\n",
        "for epoch in range(epochs):  \n",
        "\n",
        "    running_loss = 0.0\n",
        "    \n",
        "    for i, data in enumerate(target_train_loader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        # make a prediction: forward prop\n",
        "        #print(inputs.shape)\n",
        "        outputs = target_model(inputs)\n",
        "                       \n",
        "        # calculate loss\n",
        "        loss = criterion(outputs, labels)\n",
        "                       \n",
        "        # backwards propogation\n",
        "        loss.backward()       \n",
        "        \n",
        "       \n",
        "\n",
        "        # calculate gradients\n",
        "        optimizer.step()\n",
        "        # updaate weights in backprop\n",
        "                       \n",
        "                       \n",
        "        running_loss += loss.data\n",
        "        # print statistics\n",
        "        \n",
        "    ##train_accuracy = calculate_accuracy(trainloader)\n",
        "    ##test_accuracy = calculate_accuracy(testloader)\n",
        "\n",
        "    running_loss_normal = running_loss / len(target_train_loader)\n",
        "    print(\"Running Loss: %f , Normalized: %f\" % (running_loss, running_loss_normal))\n",
        "\n",
        "    #print(\"Iteration: {0} | Loss: {1} | Training accuracy: {2}% | Test accuracy: {3}%\".format(epoch+1, running_loss, train_accuracy, test_accuracy))\n",
        "        \n",
        "        \n",
        "\n",
        "print('Finished Training the Target model...')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running Loss: 184.649628 , Normalized: 1.884180\n",
            "Running Loss: 152.358749 , Normalized: 1.554681\n",
            "Running Loss: 139.334961 , Normalized: 1.421785\n",
            "Running Loss: 130.928680 , Normalized: 1.336007\n",
            "Running Loss: 121.320580 , Normalized: 1.237965\n",
            "Running Loss: 114.024734 , Normalized: 1.163518\n",
            "Running Loss: 109.153122 , Normalized: 1.113807\n",
            "Running Loss: 100.871246 , Normalized: 1.029298\n",
            "Running Loss: 98.308052 , Normalized: 1.003143\n",
            "Running Loss: 93.515930 , Normalized: 0.954244\n",
            "Running Loss: 89.133308 , Normalized: 0.909524\n",
            "Running Loss: 87.559662 , Normalized: 0.893466\n",
            "Running Loss: 82.727509 , Normalized: 0.844158\n",
            "Running Loss: 80.827721 , Normalized: 0.824773\n",
            "Running Loss: 78.740822 , Normalized: 0.803478\n",
            "Running Loss: 76.118576 , Normalized: 0.776720\n",
            "Running Loss: 73.754028 , Normalized: 0.752592\n",
            "Running Loss: 72.165146 , Normalized: 0.736379\n",
            "Running Loss: 70.028442 , Normalized: 0.714576\n",
            "Running Loss: 68.072113 , Normalized: 0.694613\n",
            "Finished Training the Target model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7w17UzPvvvRg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## OR load the model from ICP6\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qhz69K4Uw_uR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3da30f9d-a301-4b5f-c75f-56f8e7f06164"
      },
      "source": [
        "# calculate the accuracy of the Target Model\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in target_out_loader:\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = target_model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "##print(total)\n",
        "print('Accuracy of the network on the 12500 test images: %d %%' % (100 * correct / total))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 12500 test images: 69 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9B9ZGhDMw_wl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "c7d29ffd-c763-4762-8965-968721f72655"
      },
      "source": [
        "# TODO: Create shadow model(you aren't supposed to know the parameters of the target network)\n",
        "\n",
        "shadow_model = nn.Sequential(nn.Linear(3072, 128),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(128, 64),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(64,10),\n",
        "                      nn.LogSoftmax(dim=1))\n",
        "\n",
        "\n",
        "shadow_model.to(device)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=3072, out_features=128, bias=True)\n",
              "  (1): ReLU()\n",
              "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (3): ReLU()\n",
              "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
              "  (5): LogSoftmax()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNjzvkQXxX5g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "11be4e07-0b3b-4239-a12e-25aa1d303b9d"
      },
      "source": [
        "#epochs = 5\n",
        "\n",
        "# TODO: Train shadow model\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(shadow_model.parameters(), lr = .001)\n",
        "\n",
        "# let the magic begin\n",
        "for epoch in range(epochs):  \n",
        "\n",
        "    running_loss = 0.0\n",
        "    \n",
        "    for i, data in enumerate(shadow_train_loader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        inputs = inputs.view(inputs.shape[0], -1)\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        # make a prediction: forward prop\n",
        "        #print(inputs.shape)\n",
        "        outputs = shadow_model(inputs)\n",
        "                       \n",
        "        # calculate loss\n",
        "        loss = criterion(outputs, labels)\n",
        "                       \n",
        "        # backwards propogation\n",
        "        loss.backward()       \n",
        "        \n",
        "       \n",
        "\n",
        "        # calculate gradients\n",
        "        optimizer.step()\n",
        "        # updaate weights in backprop\n",
        "                       \n",
        "                       \n",
        "        running_loss += loss.data\n",
        "        # print statistics\n",
        "        \n",
        "    ##train_accuracy = calculate_accuracy(trainloader)\n",
        "    ##test_accuracy = calculate_accuracy(testloader)\n",
        "    running_loss_normal = running_loss / len(target_train_loader)\n",
        "    print(\"Running Loss: %f , Normalized: %f\" % (running_loss, running_loss_normal))\n",
        "\n",
        "    #print(\"Iteration: {0} | Loss: {1} | Training accuracy: {2}% | Test accuracy: {3}%\".format(epoch+1, running_loss, train_accuracy, test_accuracy))\n",
        "        \n",
        "        \n",
        "\n",
        "\n",
        "print('Finished Training the Shadow model')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running Loss: 191.995239 , Normalized: 1.959135\n",
            "Running Loss: 176.744125 , Normalized: 1.803512\n",
            "Running Loss: 172.310303 , Normalized: 1.758268\n",
            "Running Loss: 168.094009 , Normalized: 1.715245\n",
            "Running Loss: 165.140594 , Normalized: 1.685108\n",
            "Running Loss: 162.294724 , Normalized: 1.656069\n",
            "Running Loss: 160.590027 , Normalized: 1.638674\n",
            "Running Loss: 157.143173 , Normalized: 1.603502\n",
            "Running Loss: 156.617935 , Normalized: 1.598142\n",
            "Running Loss: 154.806244 , Normalized: 1.579656\n",
            "Running Loss: 152.818909 , Normalized: 1.559377\n",
            "Running Loss: 151.931229 , Normalized: 1.550319\n",
            "Running Loss: 151.372879 , Normalized: 1.544621\n",
            "Running Loss: 149.273178 , Normalized: 1.523196\n",
            "Running Loss: 147.931671 , Normalized: 1.509507\n",
            "Running Loss: 147.302994 , Normalized: 1.503092\n",
            "Running Loss: 147.310303 , Normalized: 1.503166\n",
            "Running Loss: 146.624176 , Normalized: 1.496165\n",
            "Running Loss: 145.421509 , Normalized: 1.483893\n",
            "Running Loss: 144.248322 , Normalized: 1.471922\n",
            "Finished Training the Shadow model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbGZ7Z8Ww_y-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "57d52488-bbfa-425a-e447-48239a2fb621"
      },
      "source": [
        "# Test the accuracy of the shadow model\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "#attack_data_set = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in shadow_out_loader:\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        inputs = inputs.view(inputs.shape[0], -1)\n",
        "        outputs = shadow_model(inputs)\n",
        "        \n",
        "        #attack_data_set.append((outputs, torch.tensor([1.0], requires_grad=True)))\n",
        "        \n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        \n",
        "    print('Accuracy of the shadow network on the shadow_train_loader: %d %%' % (100 * correct / total))\n",
        " "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the shadow network on the shadow_train_loader: 44 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCK3fFwTw_1S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make preds with shadow_model and create training data for attack model\n",
        "\n",
        "shadow_model.eval()\n",
        "\n",
        "for param in shadow_model.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "# make predictions on both datasets (shadow_in and shdow_out)\n",
        "attack_data = []\n",
        "\n",
        "for inputs, labels in shadow_train_loader:\n",
        "  inputs, labels = inputs.to(device), labels.to(device)\n",
        "  inputs = inputs.view(inputs.shape[0], -1)\n",
        "  predictions = shadow_model(inputs)\n",
        "  \n",
        "  for i in range(predictions.shape[0]):\n",
        "    attack_data.append((predictions[i], 1))\n",
        "  \n",
        "for inputs, labels in shadow_out_loader:\n",
        "  inputs, labels = inputs.to(device), labels.to(device)\n",
        "  inputs = inputs.view(inputs.shape[0], -1)\n",
        "  predictions = shadow_model(inputs)\n",
        "  \n",
        "  for i in range(predictions.shape[0]):\n",
        "    attack_data.append((predictions[i], 0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2TN556GxVZ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "attack_data_loader = torch.utils.data.DataLoader(attack_data, batch_size=4, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGf__3K3xVcZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "f8e1946b-68ad-48a6-ebaa-29b544f37555"
      },
      "source": [
        "# create the Attack Model: A NN binary classifier {0, 1}\n",
        "# the input to this model is the propability distribution vector of size 10\n",
        "# and the output is either 0 (input was not included in training) or 1\n",
        "\n",
        "\n",
        "\n",
        "epochs = 5;\n",
        "# Build a feed-forward network\n",
        "attack_model = nn.Sequential(nn.Linear(10, 128),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(128, 64),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(64, 2),\n",
        "                      nn.LogSoftmax(dim=1))\n",
        "\n",
        "attack_model.to(device)\n",
        "# Define the loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(attack_model.parameters(), lr=.001)\n",
        "\n",
        "\n",
        "for epoch in range(epochs):  \n",
        "\n",
        "    running_loss = 0.0\n",
        "    count = 0\n",
        "    for i, data in enumerate(attack_data_loader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)        \n",
        "\n",
        "        count +=1\n",
        "        #print(\"Data loader count:  %d\" % count)\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        # make a prediction: forward prop\n",
        "        #print(inputs.shape)\n",
        "        outputs = attack_model(inputs)\n",
        "                       \n",
        "        # calculate loss\n",
        "        ##print(outputs)\n",
        "        ##print(labels)\n",
        "        loss = criterion(outputs, labels)\n",
        "                       \n",
        "        # backwards propogation\n",
        "        loss.backward()       \n",
        "        \n",
        "       \n",
        "\n",
        "        # calculate gradients\n",
        "        optimizer.step()\n",
        "        # updaate weights in backprop\n",
        "                       \n",
        "                       \n",
        "        running_loss += loss.data\n",
        "        # print statistics\n",
        "        \n",
        "    ##train_accuracy = calculate_accuracy(trainloader)\n",
        "    ##test_accuracy = calculate_accuracy(testloader)\n",
        "    running_loss_normal = running_loss / len(target_train_loader)\n",
        "    print(\"Running Loss: %f , Normalized: %f\" % (running_loss, running_loss_normal))\n",
        "\n",
        "    #print(\"Iteration: {0} | Loss: {1} | Training accuracy: {2}% | Test accuracy: {3}%\".format(epoch+1, running_loss, train_accuracy, test_accuracy))\n",
        "        \n",
        "        \n",
        "\n",
        "\n",
        "print('Finished Training the Attack Model')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running Loss: 4130.949707 , Normalized: 42.152546\n",
            "Running Loss: 3970.230713 , Normalized: 40.512558\n",
            "Running Loss: 3894.924316 , Normalized: 39.744125\n",
            "Running Loss: 3858.306152 , Normalized: 39.370472\n",
            "Running Loss: 3823.988281 , Normalized: 39.020287\n",
            "Finished Training the Attack Model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybDTdj3DxhLX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Could load attack model from ICP 6, but technically it was trained in a white-box scenario"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjD2TsUkxhN8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate the recall and precision of your attack network using the Target_out and Target_in datasets\n",
        "# to do so, take a random numer of datapoints, run them throw the target model,\n",
        "# and then input the output of the target model to your attack network \n",
        "# you already know the target_in and target_out samples, so use that info to evaluate the attack model\n",
        "\n",
        "\n",
        "# Run attack agains the target model\n",
        "actual_value = []\n",
        "pred_value = []\n",
        "\n",
        "attack_model.eval()\n",
        "\n",
        "for param in attack_model.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "for inputs, labels in target_train_loader:\n",
        "  inputs, labels = inputs.to(device), labels.to(device)\n",
        "  \n",
        "  # Get probability output of target model\n",
        "  output = target_model(inputs)\n",
        "  \n",
        "  # Run through attack model\n",
        "  pred = attack_model(output)\n",
        "  \n",
        "  # Pull out top class\n",
        "  top_p, top_class = pred.topk(1, dim=1)\n",
        "  \n",
        "  for i in range(top_class.shape[0]):\n",
        "    pred_value.append(top_class[0].item())\n",
        "    actual_value.append(1)\n",
        "    \n",
        "for inputs, labels in target_out_loader:\n",
        "  inputs, labels = inputs.to(device), labels.to(device)\n",
        "  \n",
        "  # Get probability output of target model\n",
        "  output = target_model(inputs)\n",
        "  \n",
        "  # Run through attack model\n",
        "  pred = attack_model(output)\n",
        "  \n",
        "  # Pull out top class\n",
        "  top_p, top_class = pred.topk(1, dim=1)\n",
        "  \n",
        "  for i in range(top_class.shape[0]):\n",
        "    pred_value.append(top_class[0].item())\n",
        "    actual_value.append(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYop6GmGxhQI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for i in range(len(actual_value)):\n",
        " # print(\"Actual: {}   Prediction: {}\".format(actual_value[i], pred_value[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dgxb49SWxVe4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2d00c284-3d49-49f5-d826-cffb01805953"
      },
      "source": [
        "# Calculate recall and precision\n",
        "# precison = true positive / true positive + false positive\n",
        "true_pos = 0\n",
        "false_neg = 0\n",
        "\n",
        "total_positive = sum(pred_value)\n",
        "for i in range(len(actual_value)):\n",
        "  if (pred_value[i] == 1) and (actual_value[i] == 1):\n",
        "    true_pos += 1\n",
        "  elif (pred_value[i] == 0 and actual_value[i] == 1):\n",
        "    false_neg += 1\n",
        "    \n",
        "print('True positive: {} Total Positive: {} Precision: {}'.format(true_pos, total_positive, true_pos / total_positive))\n",
        "print('Recall: {}', true_pos / (true_pos + false_neg))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True positive: 11348 Total Positive: 21992 Precision: 0.5160058202982903\n",
            "Recall: {} 0.90784\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}