{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 2 - Membership Attack Possible Defense.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUFmbdXnvqCe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# required imports\n",
        "import sys \n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline  \n",
        "\n",
        "\n",
        "import torch\n",
        "import torchvision \n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data.sampler import SubsetRandomSampler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NuEdW5dvvBi",
        "colab_type": "code",
        "outputId": "423b6584-603d-4e50-86e2-0389ed790d87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# mount the google drive to download the datasets\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#project_path = 'include pathe to where your notebook is located on the drive'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5eks4_4vvDa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create transforms to load the images, nothing much is needed here. \n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "# Normalize the test set same as training set without augmentation\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vALYTeuLvvFo",
        "colab_type": "code",
        "outputId": "93a5a0bc-a918-4a17-9b22-e6d35aae2615",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# download CIFAR 10 training set\n",
        "trainset = torchvision.datasets.CIFAR10('../data', train=True,\n",
        "                                        download=True, transform=transform_train)\n",
        "\n",
        "# load the trainning set\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True)\n",
        "\n",
        "# download the test data\n",
        "testset = torchvision.datasets.CIFAR10('../data', train=True,\n",
        "                                        download=True, transform=transform_test)\n",
        "\n",
        "# load the test data\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=4, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "# check those manually on the dataset site: https://www.cs.toronto.edu/~kriz/cifar.html "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFzDOb7uvvID",
        "colab_type": "code",
        "outputId": "2ba2e22f-3ce3-4508-c48f-f2e386c2cc2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# divide the training dataset into the required groups Make sure they are balanced\n",
        "# original trainset is made of 50k images\n",
        "\n",
        "total_size = len(trainset)\n",
        "test_size = len(testset)\n",
        "#loader_size = len(trainloader)\n",
        "split1 = total_size // 4\n",
        "split2 = split1 * 2\n",
        "split3 = split1 * 3\n",
        "\n",
        "#print(loader_size)\n",
        "\n",
        "print(total_size, test_size, split1, split2, split3)\n",
        "\n",
        "indices = list(range(total_size))\n",
        "\n",
        "# two groups to train the shadow (in and out)\n",
        "shadow_train_idx = indices[:split1]\n",
        "shadow_out_idx = indices[split1:split2]\n",
        "\n",
        "# two groups to train the Target (in and out)\n",
        "target_train_idx = indices[split2:split3]\n",
        "target_out_idx = indices[split3:total_size]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000 50000 12500 25000 37500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrLiLvh0vvKh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_batch_size = 128 # pick your own\n",
        "test_batch_size = 64\n",
        "\n",
        "#train_batch_size = 128 # maybe only need one\n",
        "#test_batch_size = 64\n",
        "\n",
        "# divide and load shadow train in and out\n",
        "shadow_train_sampler = SubsetRandomSampler(shadow_train_idx) # Pytorch function\n",
        "shadow_train_loader = torch.utils.data.DataLoader(trainset, batch_size=train_batch_size, sampler=shadow_train_sampler)\n",
        "\n",
        "shadow_out_sampler = SubsetRandomSampler(shadow_out_idx)\n",
        "shadow_out_loader = torch.utils.data.DataLoader(testset, batch_size=test_batch_size, sampler=shadow_out_sampler)\n",
        "\n",
        "# divide and load Target in and out\n",
        "target_train_sampler = SubsetRandomSampler(target_train_idx)\n",
        "target_out_sampler = SubsetRandomSampler(target_out_idx)\n",
        "\n",
        "target_train_loader = torch.utils.data.DataLoader(trainset, batch_size=train_batch_size, sampler=target_train_sampler)\n",
        "\n",
        "target_out_loader = torch.utils.data.DataLoader(testset, batch_size=test_batch_size, sampler=target_out_sampler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39TpDxe8vvM9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a CNN\n",
        "# Input shape (3, 32, 32) \n",
        "# architecture: simple. 2 conv and 2 Max pool, followed by 2 fc (120, 84) \n",
        "# output of fc is 10 because we have 10 classes!\n",
        "\n",
        "\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    \"\"\"CNN.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"CNN Builder.\"\"\"\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.conv_layer = nn.Sequential(\n",
        "\n",
        "            # Conv Layer block 1\n",
        "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Conv Layer block 2\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout2d(p=0.05),\n",
        "\n",
        "        )\n",
        "\n",
        "\n",
        "        self.fc_layer = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(8192, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Perform forward.\"\"\"\n",
        "        \n",
        "        # conv layers\n",
        "        x = self.conv_layer(x)\n",
        "        \n",
        "        # flatten\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        # fc layer\n",
        "        x = self.fc_layer(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YrgFcjOvvPm",
        "colab_type": "code",
        "outputId": "d6d51004-aa35-4ef0-ad5d-7ba776c5dc5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "# initalize a target model and train it\n",
        "epochs = 20\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "target_model = CNN()\n",
        "target_model.to(device)\n",
        "criterion = nn.CrossEntropyLoss() # CrossEntropyLoss\n",
        "optimizer = optim.Adam(target_model.parameters(), lr = .001)# try Adam VS SGD\n",
        "\n",
        "# let the magic begin\n",
        "for epoch in range(epochs):  \n",
        "\n",
        "    running_loss = 0.0\n",
        "    \n",
        "    for i, data in enumerate(target_train_loader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        # make a prediction: forward prop\n",
        "        #print(inputs.shape)\n",
        "        outputs = target_model(inputs)\n",
        "                       \n",
        "        # calculate loss\n",
        "        loss = criterion(outputs, labels)\n",
        "                       \n",
        "        # backwards propogation\n",
        "        loss.backward()       \n",
        "        \n",
        "       \n",
        "\n",
        "        # calculate gradients\n",
        "        optimizer.step()\n",
        "        # updaate weights in backprop\n",
        "                       \n",
        "                       \n",
        "        running_loss += loss.data\n",
        "        # print statistics\n",
        "        \n",
        "    ##train_accuracy = calculate_accuracy(trainloader)\n",
        "    ##test_accuracy = calculate_accuracy(testloader)\n",
        "\n",
        "    running_loss_normal = running_loss / len(target_train_loader)\n",
        "    print(\"Running Loss: %f , Normalized: %f\" % (running_loss, running_loss_normal))\n",
        "\n",
        "    #print(\"Iteration: {0} | Loss: {1} | Training accuracy: {2}% | Test accuracy: {3}%\".format(epoch+1, running_loss, train_accuracy, test_accuracy))\n",
        "        \n",
        "        \n",
        "\n",
        "print('Finished Training the Target model...')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running Loss: 179.544083 , Normalized: 1.832082\n",
            "Running Loss: 149.732651 , Normalized: 1.527884\n",
            "Running Loss: 138.663406 , Normalized: 1.414933\n",
            "Running Loss: 127.310799 , Normalized: 1.299090\n",
            "Running Loss: 119.414314 , Normalized: 1.218513\n",
            "Running Loss: 112.864502 , Normalized: 1.151679\n",
            "Running Loss: 107.111084 , Normalized: 1.092970\n",
            "Running Loss: 103.676498 , Normalized: 1.057923\n",
            "Running Loss: 97.955627 , Normalized: 0.999547\n",
            "Running Loss: 95.398926 , Normalized: 0.973458\n",
            "Running Loss: 93.503601 , Normalized: 0.954118\n",
            "Running Loss: 90.151497 , Normalized: 0.919913\n",
            "Running Loss: 87.852188 , Normalized: 0.896451\n",
            "Running Loss: 83.787766 , Normalized: 0.854977\n",
            "Running Loss: 83.275620 , Normalized: 0.849751\n",
            "Running Loss: 79.758278 , Normalized: 0.813860\n",
            "Running Loss: 79.212059 , Normalized: 0.808286\n",
            "Running Loss: 78.404701 , Normalized: 0.800048\n",
            "Running Loss: 75.982094 , Normalized: 0.775327\n",
            "Running Loss: 74.323570 , Normalized: 0.758404\n",
            "Finished Training the Target model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7w17UzPvvvRg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## OR load the model from ICP6\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qhz69K4Uw_uR",
        "colab_type": "code",
        "outputId": "9f3551b4-1987-454a-dfc7-217bb04a17e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# calculate the accuracy of the Target Model\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in target_out_loader:\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = target_model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "##print(total)\n",
        "print('Accuracy of the network on the 12500 test images: %d %%' % (100 * correct / total))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 12500 test images: 68 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9B9ZGhDMw_wl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "4c456697-bcad-4dce-f683-aa31bc06c2a8"
      },
      "source": [
        "# TODO: Create shadow model(you aren't supposed to know the parameters of the target network)\n",
        "\n",
        "shadow_model = nn.Sequential(nn.Linear(3072, 128),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(128, 64),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(64,10),\n",
        "                      nn.LogSoftmax(dim=1))\n",
        "\n",
        "shadow_model.to(device)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=3072, out_features=128, bias=True)\n",
              "  (1): ReLU()\n",
              "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
              "  (3): ReLU()\n",
              "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
              "  (5): LogSoftmax()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNjzvkQXxX5g",
        "colab_type": "code",
        "outputId": "f7150fca-7403-486f-aedb-b29557fecfd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "#epochs = 5\n",
        "\n",
        "# TODO: Train shadow model\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(shadow_model.parameters(), lr = .001)\n",
        "\n",
        "# let the magic begin\n",
        "for epoch in range(epochs):  \n",
        "\n",
        "    running_loss = 0.0\n",
        "    \n",
        "    for i, data in enumerate(shadow_train_loader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        inputs = inputs.view(inputs.shape[0], -1)\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        # make a prediction: forward prop\n",
        "        #print(inputs.shape)\n",
        "        outputs = shadow_model(inputs)\n",
        "                       \n",
        "        # calculate loss\n",
        "        loss = criterion(outputs, labels)\n",
        "                       \n",
        "        # backwards propogation\n",
        "        loss.backward()       \n",
        "        \n",
        "       \n",
        "\n",
        "        # calculate gradients\n",
        "        optimizer.step()\n",
        "        # updaate weights in backprop\n",
        "                       \n",
        "                       \n",
        "        running_loss += loss.data\n",
        "        # print statistics\n",
        "        \n",
        "    ##train_accuracy = calculate_accuracy(trainloader)\n",
        "    ##test_accuracy = calculate_accuracy(testloader)\n",
        "    running_loss_normal = running_loss / len(target_train_loader)\n",
        "    print(\"Running Loss: %f , Normalized: %f\" % (running_loss, running_loss_normal))\n",
        "\n",
        "    #print(\"Iteration: {0} | Loss: {1} | Training accuracy: {2}% | Test accuracy: {3}%\".format(epoch+1, running_loss, train_accuracy, test_accuracy))\n",
        "        \n",
        "        \n",
        "\n",
        "\n",
        "print('Finished Training the Shadow model')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running Loss: 192.613922 , Normalized: 1.965448\n",
            "Running Loss: 177.780884 , Normalized: 1.814091\n",
            "Running Loss: 172.272537 , Normalized: 1.757883\n",
            "Running Loss: 167.879471 , Normalized: 1.713056\n",
            "Running Loss: 165.146027 , Normalized: 1.685163\n",
            "Running Loss: 162.393646 , Normalized: 1.657078\n",
            "Running Loss: 159.695450 , Normalized: 1.629545\n",
            "Running Loss: 157.502426 , Normalized: 1.607168\n",
            "Running Loss: 155.151932 , Normalized: 1.583183\n",
            "Running Loss: 154.765610 , Normalized: 1.579241\n",
            "Running Loss: 152.252182 , Normalized: 1.553594\n",
            "Running Loss: 150.905502 , Normalized: 1.539852\n",
            "Running Loss: 150.797546 , Normalized: 1.538750\n",
            "Running Loss: 149.363312 , Normalized: 1.524115\n",
            "Running Loss: 148.992599 , Normalized: 1.520333\n",
            "Running Loss: 147.836426 , Normalized: 1.508535\n",
            "Running Loss: 147.144455 , Normalized: 1.501474\n",
            "Running Loss: 146.327271 , Normalized: 1.493135\n",
            "Running Loss: 145.439560 , Normalized: 1.484077\n",
            "Running Loss: 144.213730 , Normalized: 1.471569\n",
            "Finished Training the Shadow model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbGZ7Z8Ww_y-",
        "colab_type": "code",
        "outputId": "c88ec3c3-8371-4d93-8d18-eacf41147d16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Test the accuracy of the shadow model\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "#attack_data_set = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in shadow_out_loader:\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        inputs = inputs.view(inputs.shape[0], -1)\n",
        "        outputs = shadow_model(inputs)\n",
        "        \n",
        "        #attack_data_set.append((outputs, torch.tensor([1.0], requires_grad=True)))\n",
        "        \n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        \n",
        "    print('Accuracy of the shadow network on the shadow_train_loader: %d %%' % (100 * correct / total))\n",
        " "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the shadow network on the shadow_train_loader: 44 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCK3fFwTw_1S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make preds with shadow_model and create training data for attack model\n",
        "\n",
        "shadow_model.eval()\n",
        "\n",
        "for param in shadow_model.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "# make predictions on both datasets (shadow_in and shdow_out)\n",
        "attack_data = []\n",
        "\n",
        "for inputs, labels in shadow_train_loader:\n",
        "  inputs, labels = inputs.to(device), labels.to(device)\n",
        "  inputs = inputs.view(inputs.shape[0], -1)\n",
        "  predictions = shadow_model(inputs)\n",
        "  \n",
        "  for i in range(predictions.shape[0]):\n",
        "    attack_data.append((predictions[i], 1))\n",
        "  \n",
        "for inputs, labels in shadow_out_loader:\n",
        "  inputs, labels = inputs.to(device), labels.to(device)\n",
        "  inputs = inputs.view(inputs.shape[0], -1)\n",
        "  predictions = shadow_model(inputs)\n",
        "  \n",
        "  for i in range(predictions.shape[0]):\n",
        "    attack_data.append((predictions[i], 0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2TN556GxVZ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "attack_data_loader = torch.utils.data.DataLoader(attack_data, batch_size=4, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGf__3K3xVcZ",
        "colab_type": "code",
        "outputId": "06fc3ead-16a9-408d-ae57-8465d905d094",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# create the Attack Model: A NN binary classifier {0, 1}\n",
        "# the input to this model is the propability distribution vector of size 10\n",
        "# and the output is either 0 (input was not included in training) or 1\n",
        "\n",
        "\n",
        "\n",
        "epochs = 5;\n",
        "# Build a feed-forward network\n",
        "attack_model = nn.Sequential(nn.Linear(10, 128),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(128, 64),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(64, 2),\n",
        "                      nn.LogSoftmax(dim=1))\n",
        "\n",
        "attack_model.to(device)\n",
        "\n",
        "# Define the loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(attack_model.parameters(), lr=.001)\n",
        "\n",
        "\n",
        "for epoch in range(epochs):  \n",
        "\n",
        "    running_loss = 0.0\n",
        "    count = 0\n",
        "    for i, data in enumerate(attack_data_loader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        count +=1\n",
        "        #print(\"Data loader count:  %d\" % count)\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        # make a prediction: forward prop\n",
        "        #print(inputs.shape)\n",
        "        outputs = attack_model(inputs)\n",
        "                       \n",
        "        # calculate loss\n",
        "        ##print(outputs)\n",
        "        ##print(labels)\n",
        "        loss = criterion(outputs, labels)\n",
        "                       \n",
        "        # backwards propogation\n",
        "        loss.backward()       \n",
        "        \n",
        "       \n",
        "\n",
        "        # calculate gradients\n",
        "        optimizer.step()\n",
        "        # updaate weights in backprop\n",
        "                       \n",
        "                       \n",
        "        running_loss += loss.data\n",
        "        # print statistics\n",
        "        \n",
        "    ##train_accuracy = calculate_accuracy(trainloader)\n",
        "    ##test_accuracy = calculate_accuracy(testloader)\n",
        "    running_loss_normal = running_loss / len(target_train_loader)\n",
        "    print(\"Running Loss: %f , Normalized: %f\" % (running_loss, running_loss_normal))\n",
        "\n",
        "    #print(\"Iteration: {0} | Loss: {1} | Training accuracy: {2}% | Test accuracy: {3}%\".format(epoch+1, running_loss, train_accuracy, test_accuracy))\n",
        "        \n",
        "        \n",
        "\n",
        "\n",
        "print('Finished Training the Attack Model')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running Loss: 4131.594238 , Normalized: 42.159122\n",
            "Running Loss: 4019.495605 , Normalized: 41.015259\n",
            "Running Loss: 3955.517578 , Normalized: 40.362423\n",
            "Running Loss: 3911.081543 , Normalized: 39.908993\n",
            "Running Loss: 3874.998047 , Normalized: 39.540794\n",
            "Finished Training the Attack Model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybDTdj3DxhLX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Could load attack model from ICP 6, but technically it was trained in a white-box scenario"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjD2TsUkxhN8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate the recall and precision of your attack network using the Target_out and Target_in datasets\n",
        "# to do so, take a random numer of datapoints, run them throw the target model,\n",
        "# and then input the output of the target model to your attack network \n",
        "# you already know the target_in and target_out samples, so use that info to evaluate the attack model\n",
        "\n",
        "\n",
        "# Run attack agains the target model\n",
        "actual_value = []\n",
        "pred_value = []\n",
        "\n",
        "attack_model.eval()\n",
        "\n",
        "for param in attack_model.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "for inputs, labels in target_train_loader:\n",
        "  inputs, labels = inputs.to(device), labels.to(device)\n",
        "  \n",
        "  # Get probability output of target model\n",
        "  output = target_model(inputs)\n",
        "  \n",
        "  # Run through attack model\n",
        "  pred = attack_model(output)\n",
        "  \n",
        "  # Pull out top class\n",
        "  top_p, top_class = pred.topk(1, dim=1)\n",
        "  \n",
        "  for i in range(top_class.shape[0]):\n",
        "    pred_value.append(top_class[0].item())\n",
        "    actual_value.append(1)\n",
        "    \n",
        "for inputs, labels in target_out_loader:\n",
        "  inputs, labels = inputs.to(device), labels.to(device)\n",
        "  \n",
        "  # Get probability output of target model\n",
        "  output = target_model(inputs)\n",
        "  \n",
        "  # Run through attack model\n",
        "  pred = attack_model(output)\n",
        "  \n",
        "  # Pull out top class\n",
        "  top_p, top_class = pred.topk(1, dim=1)\n",
        "  \n",
        "  for i in range(top_class.shape[0]):\n",
        "    pred_value.append(top_class[0].item())\n",
        "    actual_value.append(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYop6GmGxhQI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for i in range(len(actual_value)):\n",
        " # print(\"Actual: {}   Prediction: {}\".format(actual_value[i], pred_value[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dgxb49SWxVe4",
        "colab_type": "code",
        "outputId": "5e655f4d-0481-4c7f-d66b-f8dd6d81279c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Calculate recall and precision\n",
        "# precison = true positive / true positive + false positive\n",
        "true_pos = 0\n",
        "false_neg = 0\n",
        "\n",
        "total_positive = sum(pred_value)\n",
        "for i in range(len(actual_value)):\n",
        "  if (pred_value[i] == 1) and (actual_value[i] == 1):\n",
        "    true_pos += 1\n",
        "  elif (pred_value[i] == 0 and actual_value[i] == 1):\n",
        "    false_neg += 1\n",
        "    \n",
        "print('True positive: {} Total Positive: {} Precision: {}'.format(true_pos, total_positive, true_pos / total_positive))\n",
        "print('Recall: {}', true_pos / (true_pos + false_neg))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True positive: 7680 Total Positive: 16724 Precision: 0.4592202822291318\n",
            "Recall: {} 0.6144\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhkY4mleWePy",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}